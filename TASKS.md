# Implementation Guide: Local-Helix (Commerce Edition)

ì´ ë¬¸ì„œëŠ” Local-Helix í”„ë¡œì íŠ¸ì˜ ì „ì²´ êµ¬í˜„ ê³¼ì •ì„ ë‹¨ê³„ë³„ë¡œ ì„¸ë¶„í™”í•˜ì—¬ ì •ë¦¬í•œ ì‹¤í–‰ ê°€ì´ë“œì…ë‹ˆë‹¤.

---

## ğŸ“‹ Overview

**ë‚œì´ë„**: ì¤‘ìƒ  
**í•„ìˆ˜ ì„ í–‰ ì§€ì‹**: Python, SQL, ê¸°ë³¸ ML ê°œë…

---

## Phase 0: í”„ë¡œì íŠ¸ ê¸°íš ë° ì¤€ë¹„ âœ…

### ì™„ë£Œëœ ì‘ì—…
- [x] PRD.md ì‘ì„±
- [x] TASKS.md ì‘ì„±
- [x] TECH_STACK.md ì‘ì„±
- [x] README.md ì‘ì„±
- [x] GitHub ì´ìŠˆ ìƒì„± ìŠ¤í¬ë¦½íŠ¸ ê°œë°œ

### ë‹¤ìŒ ë‹¨ê³„
- [ ] GitHub Issues ìƒì„± ì‹¤í–‰
- [ ] í”„ë¡œì íŠ¸ ë””ë ‰í† ë¦¬ êµ¬ì¡° ìƒì„±

---

## Phase 1: í™˜ê²½ ì„¤ì •

### ëª©í‘œ
ë¡œì»¬ ê°œë°œ í™˜ê²½ì„ êµ¬ì¶•í•˜ê³  ëª¨ë“  í•„ìˆ˜ ë„êµ¬ë¥¼ ì„¤ì¹˜í•©ë‹ˆë‹¤.

### ì„¸ë¶€ ì‘ì—…

#### 1.1 Python í™˜ê²½ ì„¤ì •
```bash
# Python ë²„ì „ í™•ì¸
python --version  # 3.10 ì´ìƒ í•„ìš”

# ê°€ìƒí™˜ê²½ ìƒì„± (ê¶Œì¥)
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate
```

#### 1.2 í•µì‹¬ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜
```bash
# requirements.txt ìƒì„±
cat > requirements.txt << EOF
duckdb>=0.9.0
polars>=0.20.0
lightgbm>=4.0.0
mlflow>=2.10.0
scipy>=1.11.0
statsmodels>=0.14.0
streamlit>=1.30.0
matplotlib>=3.7.0
seaborn>=0.12.0
jupyter>=1.0.0
requests>=2.31.0
kaggle>=1.5.0
EOF

# ì„¤ì¹˜
pip install -r requirements.txt
```

#### 1.3 Ollama ì„¤ì¹˜ ë° ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
```bash
# Ollama ì„¤ì¹˜ (https://ollama.ai)
# Windows: ì„¤ì¹˜ í”„ë¡œê·¸ë¨ ë‹¤ìš´ë¡œë“œ
# macOS: brew install ollama
# Linux: curl -fsSL https://ollama.ai/install.sh | sh

# Llama 3 ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
ollama pull llama3
```

#### 1.4 Kaggle ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ
```bash
# Kaggle API ì„¤ì •
mkdir -p ~/.kaggle
# kaggle.json íŒŒì¼ì„ ~/.kaggle/ì— ë³µì‚¬

# H&M ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ
kaggle competitions download -c h-and-m-personalized-fashion-recommendations
unzip h-and-m-personalized-fashion-recommendations.zip -d data/raw/
```

#### 1.5 í”„ë¡œì íŠ¸ ë””ë ‰í† ë¦¬ êµ¬ì¡° ìƒì„±
```bash
mkdir -p data/{raw,processed,features}
mkdir -p notebooks
mkdir -p src/{data,models,simulation,utils}
mkdir -p models/artifacts
mkdir -p logs
mkdir -p reports
```

**ì²´í¬í¬ì¸íŠ¸**: DuckDB ì—°ê²° í…ŒìŠ¤íŠ¸ ì„±ê³µ

---

## Phase 2: ë°ì´í„° ì—”ì§€ë‹ˆì–´ë§

### ëª©í‘œ
3ì²œë§Œ ê±´ì˜ íŠ¸ëœì­ì…˜ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ê³  User/Item Featureë¥¼ ìƒì„±í•©ë‹ˆë‹¤.

### ì„¸ë¶€ ì‘ì—…

#### 2.1 ë°ì´í„° íƒìƒ‰ (EDA)
**íŒŒì¼**: `notebooks/01_eda.ipynb`

```python
import duckdb
import polars as pl

# DuckDB ì—°ê²°
con = duckdb.connect('local_helix.db')

# ë°ì´í„° ë¡œë“œ ë° ê¸°ë³¸ í†µê³„
con.execute("""
    SELECT COUNT(*) as total_transactions
    FROM 'data/raw/transactions_train.csv'
""").fetchall()

# ìœ ì € ìˆ˜, ìƒí’ˆ ìˆ˜ í™•ì¸
# ì‹œê³„ì—´ ë¶„í¬ í™•ì¸
# ì¹´í…Œê³ ë¦¬ ë¶„í¬ í™•ì¸
```

**ì²´í¬ë¦¬ìŠ¤íŠ¸**:
- [ ] ì „ì²´ íŠ¸ëœì­ì…˜ ìˆ˜ í™•ì¸
- [ ] ìœ ë‹ˆí¬ ìœ ì € ìˆ˜ í™•ì¸
- [ ] ìœ ë‹ˆí¬ ìƒí’ˆ ìˆ˜ í™•ì¸
- [ ] ì‹œê³„ì—´ ë²”ìœ„ í™•ì¸
- [ ] ê²°ì¸¡ì¹˜ ë¶„ì„
- [ ] ì´ìƒì¹˜ íƒì§€

#### 2.2 User Feature ìƒì„±
**íŒŒì¼**: `src/data/user_features.py`

```python
def create_user_features(con):
    """
    ìœ ì €ë³„ Feature ìƒì„±:
    - avg_purchase_hour: ìµœê·¼ 4ì£¼ í‰ê·  êµ¬ë§¤ ì‹œê°„ëŒ€
    - preferred_category: ì„ í˜¸ ì¹´í…Œê³ ë¦¬
    - recency: ë§ˆì§€ë§‰ êµ¬ë§¤ë¡œë¶€í„° ê²½ê³¼ì¼
    - purchase_frequency: êµ¬ë§¤ ë¹ˆë„
    """
    query = """
    CREATE OR REPLACE TABLE user_features AS
    SELECT 
        customer_id,
        AVG(EXTRACT(HOUR FROM t_dat)) as avg_purchase_hour,
        MODE(product_type_no) as preferred_category,
        DATEDIFF('day', MAX(t_dat), CURRENT_DATE) as recency,
        COUNT(*) as purchase_count
    FROM 'data/raw/transactions_train.csv'
    WHERE t_dat >= CURRENT_DATE - INTERVAL '28 days'
    GROUP BY customer_id
    """
    con.execute(query)
    
    # Parquetë¡œ ì €ì¥
    con.execute("""
        COPY user_features TO 'data/features/user_features.parquet'
        (FORMAT PARQUET)
    """)
```

**ì²´í¬ë¦¬ìŠ¤íŠ¸**:
- [ ] avg_purchase_hour ê³„ì‚°
- [ ] preferred_category ì¶”ì¶œ
- [ ] recency ê³„ì‚°
- [ ] purchase_frequency ê³„ì‚°
- [ ] Feature ê²€ì¦ (NULL ì²´í¬)
- [ ] Parquet ì €ì¥ í™•ì¸

#### 2.3 Item Feature ìƒì„±
**íŒŒì¼**: `src/data/item_features.py`

```python
def create_item_features(con):
    """
    ìƒí’ˆë³„ Feature ìƒì„±:
    - popularity_rank: ìµœê·¼ 1ì£¼ íŒë§¤ëŸ‰ ìˆœìœ„
    - peak_hour: ì£¼ íŒë§¤ ì‹œê°„ëŒ€
    - avg_price: í‰ê·  ê°€ê²©
    """
    query = """
    CREATE OR REPLACE TABLE item_features AS
    SELECT 
        article_id,
        RANK() OVER (ORDER BY COUNT(*) DESC) as popularity_rank,
        MODE(EXTRACT(HOUR FROM t_dat)) as peak_hour,
        COUNT(*) as sales_count
    FROM 'data/raw/transactions_train.csv'
    WHERE t_dat >= CURRENT_DATE - INTERVAL '7 days'
    GROUP BY article_id
    """
    con.execute(query)
```

**ì²´í¬ë¦¬ìŠ¤íŠ¸**:
- [ ] popularity_rank ê³„ì‚°
- [ ] peak_hour ì¶”ì¶œ
- [ ] sales_count ì§‘ê³„
- [ ] Feature ê²€ì¦
- [ ] Parquet ì €ì¥

#### 2.4 Feature Store êµ¬ì¶•
**íŒŒì¼**: `src/data/feature_store.py`

```python
class FeatureStore:
    def __init__(self, db_path='local_helix.db'):
        self.con = duckdb.connect(db_path)
    
    def get_user_features(self, user_ids):
        """ìœ ì € Feature ì¡°íšŒ"""
        pass
    
    def get_item_features(self, item_ids):
        """ìƒí’ˆ Feature ì¡°íšŒ"""
        pass
    
    def refresh_features(self):
        """Feature ì¬ìƒì„±"""
        pass
```

**ì²´í¬í¬ì¸íŠ¸**: Feature Parquet íŒŒì¼ ìƒì„± ì™„ë£Œ

---

## Phase 3: í›„ë³´êµ° ìƒì„±

### ëª©í‘œ
ì „ì²´ ìƒí’ˆ ì¤‘ ìœ ì €ë³„ë¡œ ì¶”ì²œ ê°€ëŠ¥í•œ í›„ë³´êµ°ì„ ìƒì„±í•©ë‹ˆë‹¤.

### ì„¸ë¶€ ì‘ì—…

#### 3.1 Popularity ê¸°ë°˜ í›„ë³´êµ°
**íŒŒì¼**: `src/models/candidate_generation.py`

```python
def generate_popularity_candidates(con, top_k=50):
    """
    ì „ì²´ ìœ ì € ëŒ€ìƒ ì¸ê¸° ìƒí’ˆ Top K ì¶”ì¶œ
    """
    query = """
    SELECT article_id, sales_count
    FROM item_features
    ORDER BY popularity_rank
    LIMIT ?
    """
    return con.execute(query, [top_k]).fetchall()
```

#### 3.2 Item-based Collaborative Filtering
**íŒŒì¼**: `src/models/candidate_generation.py`

```python
def generate_cf_candidates(con, user_id, top_k=50):
    """
    ìœ ì €ì˜ ìµœê·¼ êµ¬ë§¤ ìƒí’ˆê³¼ ìœ ì‚¬í•œ ìƒí’ˆ ì¶”ì²œ
    Co-visitation ê¸°ë°˜
    """
    # 1. ìœ ì €ì˜ ìµœê·¼ Nê°œ êµ¬ë§¤ ìƒí’ˆ ì¡°íšŒ
    query_user_items = """
    SELECT DISTINCT article_id
    FROM 'data/raw/transactions_train.csv'
    WHERE customer_id = ?
    ORDER BY t_dat DESC
    LIMIT 10
    """
    user_items = con.execute(query_user_items, [user_id]).fetchall()
    
    # 2. Co-visitation í–‰ë ¬ ê³„ì‚°
    query_similar = """
    SELECT 
        t2.article_id,
        COUNT(DISTINCT t2.customer_id) as covisit_count
    FROM 'data/raw/transactions_train.csv' t1
    JOIN 'data/raw/transactions_train.csv' t2
        ON t1.customer_id = t2.customer_id
        AND t1.article_id != t2.article_id
    WHERE t1.article_id IN (?)
    GROUP BY t2.article_id
    ORDER BY covisit_count DESC
    LIMIT ?
    """
    return con.execute(query_similar, [user_items, top_k]).fetchall()
```

**ì²´í¬ë¦¬ìŠ¤íŠ¸**:
- [ ] ìœ ì €ì˜ ìµœê·¼ Nê°œ êµ¬ë§¤ ìƒí’ˆ ì¡°íšŒ (N=10 ê¶Œì¥)
- [ ] Co-visitation í–‰ë ¬ ê³„ì‚° (ìƒí’ˆ Aì™€ Bë¥¼ í•¨ê»˜ êµ¬ë§¤í•œ ìœ ì € ìˆ˜)
- [ ] ìœ ì‚¬ë„ ì ìˆ˜ ì •ê·œí™” (ì„ íƒ ì‚¬í•­)
- [ ] Top K ìœ ì‚¬ ìƒí’ˆ ì¶”ì¶œ
- [ ] ì´ë¯¸ êµ¬ë§¤í•œ ìƒí’ˆ ì œì™¸
- [ ] ê²°ê³¼ ìºì‹± êµ¬í˜„ (ì„±ëŠ¥ ìµœì í™”)

#### 3.3 í›„ë³´êµ° ë³‘í•©
**íŒŒì¼**: `src/models/candidate_generation.py`

```python
def merge_candidates(con, user_id, total_k=100):
    """
    Popularity + CF í›„ë³´êµ° ë³‘í•©
    ìµœì¢… 50~100ê°œ í›„ë³´êµ° ìƒì„±
    """
    # 1. Popularity í›„ë³´êµ° (50%)
    popularity_items = generate_popularity_candidates(con, top_k=50)
    
    # 2. CF í›„ë³´êµ° (50%)
    cf_items = generate_cf_candidates(con, user_id, top_k=50)
    
    # 3. ì¤‘ë³µ ì œê±°
    all_candidates = set([item[0] for item in popularity_items])
    all_candidates.update([item[0] for item in cf_items])
    
    # 4. ë‹¤ì–‘ì„± í™•ë³´ (ì¹´í…Œê³ ë¦¬ ë¶„ì‚°)
    # í•„ìš”ì‹œ ì¹´í…Œê³ ë¦¬ë³„ ì¿¼í„° ì ìš©
    
    return list(all_candidates)[:total_k]
```

**ì²´í¬ë¦¬ìŠ¤íŠ¸**:
- [ ] Popularityì™€ CF í›„ë³´êµ° ë¹„ìœ¨ ê²°ì • (50:50 ê¶Œì¥)
- [ ] ì¤‘ë³µ ìƒí’ˆ ì œê±°
- [ ] ì¹´í…Œê³ ë¦¬ ë‹¤ì–‘ì„± í™•ë³´
- [ ] ìµœì¢… í›„ë³´êµ° í¬ê¸° ê²€ì¦ (50~100ê°œ)
- [ ] í›„ë³´êµ° í’ˆì§ˆ ê²€ì¦ (NULL ì²´í¬)

**ì²´í¬í¬ì¸íŠ¸**: ìœ ì €ë³„ í›„ë³´êµ° ìƒì„± ì™„ë£Œ

---

## Phase 4: ë­í‚¹ ëª¨ë¸

### ëª©í‘œ
LightGBMì„ í™œìš©í•˜ì—¬ êµ¬ë§¤ í™•ë¥  ì˜ˆì¸¡ ëª¨ë¸ì„ í•™ìŠµí•©ë‹ˆë‹¤.

### ì„¸ë¶€ ì‘ì—…

#### 4.1 í•™ìŠµ ë°ì´í„°ì…‹ êµ¬ì„±
**íŒŒì¼**: `src/models/dataset.py`

```python
def create_training_dataset(con):
    """
    (User, Item, Time) Feature + Label ë³‘í•©
    Positive: ì‹¤ì œ êµ¬ë§¤ (1)
    Negative: í›„ë³´êµ° ì¤‘ ë¯¸êµ¬ë§¤ (0)
    """
    query = """
    SELECT 
        u.customer_id,
        i.article_id,
        u.avg_purchase_hour,
        u.preferred_category,
        u.recency,
        i.popularity_rank,
        i.peak_hour,
        CASE WHEN t.article_id IS NOT NULL THEN 1 ELSE 0 END as label
    FROM user_features u
    CROSS JOIN item_features i
    LEFT JOIN transactions t 
        ON u.customer_id = t.customer_id 
        AND i.article_id = t.article_id
    """
    return con.execute(query).fetch_df()
```

#### 4.2 Train/Validation Split
```python
from sklearn.model_selection import TimeSeriesSplit

# ì‹œê³„ì—´ ê¸°ë°˜ ë¶„í• 
tscv = TimeSeriesSplit(n_splits=5)
for train_idx, val_idx in tscv.split(X):
    X_train, X_val = X[train_idx], X[val_idx]
    y_train, y_val = y[train_idx], y[val_idx]
```

#### 4.3 LightGBM í•™ìŠµ
**íŒŒì¼**: `src/models/ranker.py`

```python
import lightgbm as lgb
import mlflow

def train_ranker(X_train, y_train, X_val, y_val):
    """
    LightGBM Ranker í•™ìŠµ
    """
    mlflow.start_run()
    
    params = {
        'objective': 'lambdarank',
        'metric': 'ndcg',
        'boosting_type': 'gbdt',
        'num_leaves': 31,
        'learning_rate': 0.05,
        'feature_fraction': 0.9
    }
    
    train_data = lgb.Dataset(X_train, label=y_train)
    val_data = lgb.Dataset(X_val, label=y_val, reference=train_data)
    
    model = lgb.train(
        params,
        train_data,
        num_boost_round=100,
        valid_sets=[val_data],
        callbacks=[lgb.early_stopping(10)]
    )
    
    # MLflow ë¡œê¹…
    mlflow.log_params(params)
    mlflow.log_metric('ndcg', model.best_score['valid_0']['ndcg'])
    mlflow.lightgbm.log_model(model, 'model')
    
    return model
```

#### 4.4 ëª¨ë¸ í‰ê°€
**íŒŒì¼**: `src/models/evaluation.py`

```python
import numpy as np
from sklearn.metrics import ndcg_score

def evaluate_model(model, X_val, y_val, user_groups, k_values=[10, 20, 50]):
    """
    ë‹¤ì–‘í•œ ë©”íŠ¸ë¦­ìœ¼ë¡œ ëª¨ë¸ í‰ê°€
    """
    predictions = model.predict(X_val)
    
    metrics = {}
    for k in k_values:
        # Hit Rate@K
        hit_rate = calculate_hit_rate(predictions, y_val, user_groups, k)
        metrics[f'hit_rate@{k}'] = hit_rate
        
        # NDCG@K
        ndcg = calculate_ndcg(predictions, y_val, user_groups, k)
        metrics[f'ndcg@{k}'] = ndcg
        
        # Precision@K
        precision = calculate_precision(predictions, y_val, user_groups, k)
        metrics[f'precision@{k}'] = precision
    
    return metrics

def calculate_hit_rate(predictions, y_true, user_groups, k):
    """
    Hit Rate@K: ì¶”ì²œ Kê°œ ì¤‘ ì‹¤ì œ êµ¬ë§¤ê°€ í¬í•¨ëœ ë¹„ìœ¨
    """
    hits = 0
    total_users = len(user_groups)
    
    for user_id, indices in user_groups.items():
        user_preds = predictions[indices]
        user_labels = y_true[indices]
        
        # Top K ì¶”ì¶œ
        top_k_idx = np.argsort(user_preds)[-k:]
        top_k_labels = user_labels[top_k_idx]
        
        # ì‹¤ì œ êµ¬ë§¤ê°€ í•˜ë‚˜ë¼ë„ í¬í•¨ë˜ë©´ hit
        if np.sum(top_k_labels) > 0:
            hits += 1
    
    return hits / total_users
```

**ì²´í¬ë¦¬ìŠ¤íŠ¸**:
- [ ] ê²€ì¦ ë°ì´í„°ì…‹ì— ëŒ€í•œ ì˜ˆì¸¡ ìˆ˜í–‰
- [ ] Top K ì¶”ì²œ ë¦¬ìŠ¤íŠ¸ ìƒì„± (K=10, 20, 50)
- [ ] Hit Rate@K ê³„ì‚°
- [ ] NDCG@K ê³„ì‚°
- [ ] Precision@K, Recall@K ê³„ì‚°
- [ ] í‰ê°€ ê²°ê³¼ ì‹œê°í™” (matplotlib)
- [ ] MLflowì— ëª¨ë“  ë©”íŠ¸ë¦­ ë¡œê¹…
- [ ] Feature Importance ë¶„ì„ ë° ì €ì¥

**ì²´í¬í¬ì¸íŠ¸**: MLflowì— ëª¨ë¸ ì €ì¥ ì™„ë£Œ

---

## Phase 5: ë¡œì»¬ ì„œë¹™

### ëª©í‘œ
í•™ìŠµëœ ëª¨ë¸ì„ í™œìš©í•˜ì—¬ ì‹¤ì‹œê°„ ì¶”ì²œ APIë¥¼ êµ¬í˜„í•©ë‹ˆë‹¤.

### ì„¸ë¶€ ì‘ì—…

#### 5.1 Serving í•¨ìˆ˜ êµ¬í˜„
**íŒŒì¼**: `src/models/serving.py`

```python
import polars as pl

def merge_features(user_features, item_features):
    """
    Userì™€ Item Featureë¥¼ ë³‘í•©í•˜ì—¬ ëª¨ë¸ ì…ë ¥ ìƒì„±
    """
    # Polars DataFrameìœ¼ë¡œ ë³€í™˜
    user_df = pl.DataFrame(user_features)
    item_df = pl.DataFrame(item_features)
    
    # Cross joinìœ¼ë¡œ ëª¨ë“  ì¡°í•© ìƒì„±
    merged = user_df.join(item_df, how='cross')
    
    # í•„ìš”í•œ ì»¬ëŸ¼ë§Œ ì„ íƒ
    feature_cols = [
        'avg_purchase_hour', 'preferred_category', 'recency',
        'popularity_rank', 'peak_hour', 'sales_count'
    ]
    
    return merged.select(feature_cols).to_numpy()

class RecommendationService:
    def __init__(self, model_path):
        self.model = lgb.Booster(model_file=model_path)
        self.feature_store = FeatureStore()
    
    def predict(self, user_id, top_k=10):
        """
        ìœ ì €ë³„ Top K ì¶”ì²œ ìƒí’ˆ ë° ìµœì  ë°œì†¡ ì‹œê°„ ë°˜í™˜
        """
        # 1. í›„ë³´êµ° ìƒì„±
        candidates = merge_candidates(user_id)
        
        # 2. Feature ì¡°íšŒ
        user_features = self.feature_store.get_user_features([user_id])
        item_features = self.feature_store.get_item_features(candidates)
        
        # 3. ì˜ˆì¸¡
        X = merge_features(user_features, item_features)
        scores = self.model.predict(X)
        
        # 4. Top K ì¶”ì¶œ
        top_items = candidates[scores.argsort()[-top_k:]]
        
        # 5. ìµœì  ë°œì†¡ ì‹œê°„ ê³„ì‚°
        optimal_hour = user_features['avg_purchase_hour']
        
        return {
            'user_id': user_id,
            'recommendations': top_items,
            'optimal_send_time': optimal_hour
        }
```

#### 5.2 ë°°ì¹˜ ì¶”ë¡ 
**íŒŒì¼**: `scripts/batch_inference.py`

```python
import pandas as pd
from tqdm import tqdm
import logging

def batch_inference(user_ids, output_path='data/recommendations.csv', batch_size=100):
    """
    ì „ì²´ íƒ€ê²Ÿ ìœ ì €ì— ëŒ€í•œ ì¶”ì²œ ìƒì„±
    """
    logging.basicConfig(level=logging.INFO)
    logger = logging.getLogger(__name__)
    
    service = RecommendationService('models/artifacts/model.txt')
    
    results = []
    failed_users = []
    
    # ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì²˜ë¦¬
    for i in tqdm(range(0, len(user_ids), batch_size)):
        batch = user_ids[i:i+batch_size]
        
        for user_id in batch:
            try:
                result = service.predict(user_id)
                
                # ê²°ê³¼ ê²€ì¦
                if result and result['recommendations']:
                    results.append(result)
                else:
                    logger.warning(f"Empty result for user {user_id}")
                    
            except Exception as e:
                logger.error(f"Failed for user {user_id}: {str(e)}")
                failed_users.append(user_id)
    
    # ê²°ê³¼ ì €ì¥
    df = pd.DataFrame(results)
    df.to_csv(output_path, index=False)
    
    # ì‹¤íŒ¨í•œ ìœ ì € ë¡œê¹…
    if failed_users:
        with open('logs/failed_users.txt', 'w') as f:
            f.write('\n'.join(map(str, failed_users)))
    
    logger.info(f"Completed: {len(results)} success, {len(failed_users)} failed")
```

**ì²´í¬ë¦¬ìŠ¤íŠ¸**:
- [ ] íƒ€ê²Ÿ ìœ ì € ë¦¬ìŠ¤íŠ¸ ë¡œë“œ
- [ ] ë°°ì¹˜ í¬ê¸° ì„¤ì • (ë©”ëª¨ë¦¬ ê´€ë¦¬, 100~1000 ê¶Œì¥)
- [ ] ì§„í–‰ë¥  í‘œì‹œ (tqdm)
- [ ] ì—ëŸ¬ í•¸ë“¤ë§ ë° ì‹¤íŒ¨ ìœ ì € ë¡œê¹…
- [ ] ê²°ê³¼ ê²€ì¦ (NULL, ë¹ˆ ë¦¬ìŠ¤íŠ¸ ì²´í¬)
- [ ] CSV ì €ì¥ ë° ë°±ì—…
- [ ] ì²˜ë¦¬ ì‹œê°„ ì¸¡ì • ë° ë¡œê¹…

**ì²´í¬í¬ì¸íŠ¸**: ë°°ì¹˜ ì¶”ë¡  ê²°ê³¼ íŒŒì¼ ìƒì„±

---

## Phase 6: ê°€ìƒ ìœ ì € ì‹œë®¬ë ˆì´ì…˜

### ëª©í‘œ
Ollamaë¥¼ í™œìš©í•˜ì—¬ A/B í…ŒìŠ¤íŠ¸ ì‹œë®¬ë ˆì´ì…˜ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.

### ì„¸ë¶€ ì‘ì—…

#### 6.1 Ollama API ì—°ë™
**íŒŒì¼**: `src/simulation/llm_client.py`

```python
import requests

class OllamaClient:
    def __init__(self, base_url='http://localhost:11434'):
        self.base_url = base_url
    
    def generate(self, prompt, model='llama3'):
        response = requests.post(
            f'{self.base_url}/api/generate',
            json={'model': model, 'prompt': prompt}
        )
        return response.json()
```

#### 6.2 í˜ë¥´ì†Œë‚˜ ìƒì„±
**íŒŒì¼**: `src/simulation/persona.py`

```python
from dataclasses import dataclass
from typing import List

@dataclass
class UserMetadata:
    """ìœ ì € ë©”íƒ€ë°ì´í„° ìŠ¤í‚¤ë§ˆ"""
    user_id: str
    age_group: str  # '20s', '30s', '40s', etc.
    preferred_category: str
    avg_purchase_hour: int
    recent_purchases: List[str]
    purchase_frequency: str  # 'high', 'medium', 'low'

def create_persona(user_metadata: UserMetadata) -> str:
    """
    ìœ ì € ë©”íƒ€ë°ì´í„° ê¸°ë°˜ í˜ë¥´ì†Œë‚˜ í”„ë¡¬í”„íŠ¸ ìƒì„±
    """
    # ë‚˜ì´ëŒ€ë³„ í˜ë¥´ì†Œë‚˜ íŠ¹ì„±
    age_traits = {
        '20s': 'íŠ¸ë Œë””í•˜ê³  ìƒˆë¡œìš´ ìŠ¤íƒ€ì¼ì„ ì¶”êµ¬í•˜ëŠ”',
        '30s': 'ì‹¤ìš©ì ì´ë©´ì„œë„ í’ˆì§ˆì„ ì¤‘ì‹œí•˜ëŠ”',
        '40s': 'í´ë˜ì‹í•˜ê³  ì•ˆì •ì ì¸ ìŠ¤íƒ€ì¼ì„ ì„ í˜¸í•˜ëŠ”',
    }
    
    # êµ¬ë§¤ ë¹ˆë„ë³„ íŠ¹ì„±
    frequency_traits = {
        'high': 'íŒ¨ì…˜ì— ê´€ì‹¬ì´ ë§ì•„ ìì£¼ ì‡¼í•‘í•˜ëŠ”',
        'medium': 'í•„ìš”í•  ë•Œ ê³„íšì ìœ¼ë¡œ êµ¬ë§¤í•˜ëŠ”',
        'low': 'ì‹ ì¤‘í•˜ê²Œ ì„ íƒí•˜ì—¬ ê°€ë” êµ¬ë§¤í•˜ëŠ”',
    }
    
    template = f"""
    ë‹¹ì‹ ì€ ë‹¤ìŒê³¼ ê°™ì€ íŠ¹ì„±ì„ ê°€ì§„ H&M ê³ ê°ì…ë‹ˆë‹¤:
    
    **ê¸°ë³¸ ì •ë³´**:
    - ì—°ë ¹ëŒ€: {user_metadata.age_group}
    - ì‡¼í•‘ ì„±í–¥: {age_traits.get(user_metadata.age_group, 'ì¼ë°˜ì ì¸')} {frequency_traits.get(user_metadata.purchase_frequency, 'ê³ ê°')}
    
    **ì„ í˜¸ë„**:
    - ì£¼ë¡œ êµ¬ë§¤í•˜ëŠ” ì¹´í…Œê³ ë¦¬: {user_metadata.preferred_category}
    - ì„ í˜¸ ì‡¼í•‘ ì‹œê°„ëŒ€: {user_metadata.avg_purchase_hour}ì‹œê²½
    - ìµœê·¼ êµ¬ë§¤ ìƒí’ˆ: {', '.join(user_metadata.recent_purchases[:3])}
    
    ë‹¹ì‹ ì˜ ì‡¼í•‘ ì„±í–¥ê³¼ ì„ í˜¸ë„ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì†”ì§í•˜ê²Œ í–‰ë™í•´ì£¼ì„¸ìš”.
    """
    
    return template

def load_user_metadata(con, user_id):
    """DuckDBì—ì„œ ìœ ì € ë©”íƒ€ë°ì´í„° ë¡œë“œ"""
    query = """
    SELECT 
        customer_id,
        CASE 
            WHEN age < 30 THEN '20s'
            WHEN age < 40 THEN '30s'
            ELSE '40s'
        END as age_group,
        preferred_category,
        avg_purchase_hour,
        recent_purchases,
        CASE
            WHEN purchase_count > 10 THEN 'high'
            WHEN purchase_count > 5 THEN 'medium'
            ELSE 'low'
        END as purchase_frequency
    FROM user_features
    WHERE customer_id = ?
    """
    result = con.execute(query, [user_id]).fetchone()
    return UserMetadata(*result) if result else None
```

**ì²´í¬ë¦¬ìŠ¤íŠ¸**:
- [ ] ìœ ì € ë©”íƒ€ë°ì´í„° ìŠ¤í‚¤ë§ˆ ì •ì˜ (UserMetadata dataclass)
- [ ] ë‚˜ì´ëŒ€ë³„ í˜ë¥´ì†Œë‚˜ íŠ¹ì„± ì •ì˜
- [ ] êµ¬ë§¤ ë¹ˆë„ë³„ íŠ¹ì„± ì •ì˜
- [ ] ì¹´í…Œê³ ë¦¬ë³„ ì„ í˜¸ë„ í”„ë¡¬í”„íŠ¸ ì‘ì„±
- [ ] í˜ë¥´ì†Œë‚˜ í…œí”Œë¦¿ ê²€ì¦ (ë‹¤ì–‘í•œ ìœ ì € íƒ€ì…)
- [ ] LLM ì‘ë‹µ í’ˆì§ˆ í…ŒìŠ¤íŠ¸
- [ ] í˜ë¥´ì†Œë‚˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ êµ¬ì¶• (ì¬ì‚¬ìš©)

#### 6.3 A/B í…ŒìŠ¤íŠ¸ ì‹œë®¬ë ˆì´í„°
**íŒŒì¼**: `src/simulation/ab_test.py`

```python
class ABTestSimulator:
    def __init__(self, llm_client):
        self.llm = llm_client
    
    def simulate_group_a(self, user_id):
        """Control: ì¸ê¸° ìƒí’ˆ + ëœë¤ ì‹œê°„"""
        popular_items = generate_popularity_candidates(top_k=5)
        send_time = random.randint(9, 21)
        
        prompt = f"""
        {create_persona(user_id)}
        
        ì˜¤ëŠ˜ {send_time}ì‹œì— ë‹¤ìŒ ìƒí’ˆ ì¶”ì²œ í‘¸ì‹œë¥¼ ë°›ì•˜ìŠµë‹ˆë‹¤:
        {popular_items}
        
        ì´ í‘¸ì‹œë¥¼ í´ë¦­í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (Yes/Noë¡œë§Œ ë‹µë³€)
        """
        
        response = self.llm.generate(prompt)
        return 'yes' in response.lower()
    
    def simulate_group_b(self, user_id):
        """Test: ëª¨ë¸ ì¶”ì²œ + ìµœì  ì‹œê°„"""
        service = RecommendationService('models/artifacts/model.txt')
        result = service.predict(user_id)
        
        prompt = f"""
        {create_persona(user_id)}
        
        ì˜¤ëŠ˜ {result['optimal_send_time']}ì‹œì— ë‹¤ìŒ ìƒí’ˆ ì¶”ì²œ í‘¸ì‹œë¥¼ ë°›ì•˜ìŠµë‹ˆë‹¤:
        {result['recommendations']}
        
        ì´ í‘¸ì‹œë¥¼ í´ë¦­í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (Yes/Noë¡œë§Œ ë‹µë³€)
        """
        
        response = self.llm.generate(prompt)
        return 'yes' in response.lower()
```

#### 6.4 ì‹œë®¬ë ˆì´ì…˜ ì‹¤í–‰
**íŒŒì¼**: `scripts/run_simulation.py`

```python
import random
import pandas as pd
from tqdm import tqdm
import logging

def sample_users(con, n_users=1000, sampling_method='random'):
    """
    ì‹œë®¬ë ˆì´ì…˜ ëŒ€ìƒ ìœ ì € ìƒ˜í”Œë§
    """
    if sampling_method == 'random':
        query = """
        SELECT customer_id
        FROM user_features
        ORDER BY RANDOM()
        LIMIT ?
        """
    elif sampling_method == 'stratified':
        # êµ¬ë§¤ ë¹ˆë„ë³„ ê³„ì¸µ ìƒ˜í”Œë§
        query = """
        WITH stratified AS (
            SELECT 
                customer_id,
                ROW_NUMBER() OVER (PARTITION BY purchase_frequency ORDER BY RANDOM()) as rn
            FROM user_features
        )
        SELECT customer_id
        FROM stratified
        WHERE rn <= ?
        """
    
    return [row[0] for row in con.execute(query, [n_users]).fetchall()]

def run_ab_test(con, n_users=1000, sampling_method='random'):
    """
    A/B í…ŒìŠ¤íŠ¸ ì‹œë®¬ë ˆì´ì…˜ ì‹¤í–‰
    """
    logging.basicConfig(level=logging.INFO)
    logger = logging.getLogger(__name__)
    
    simulator = ABTestSimulator(OllamaClient())
    user_ids = sample_users(con, n_users, sampling_method)
    
    results = []
    logger.info(f"Starting A/B test simulation with {n_users} users")
    
    for user_id in tqdm(user_ids, desc="Simulating users"):
        # ëœë¤ ê·¸ë£¹ í• ë‹¹ (50:50)
        group = 'A' if random.random() < 0.5 else 'B'
        
        try:
            if group == 'A':
                clicked = simulator.simulate_group_a(user_id)
            else:
                clicked = simulator.simulate_group_b(user_id)
            
            results.append({
                'user_id': user_id,
                'group': group,
                'clicked': clicked,
                'timestamp': pd.Timestamp.now()
            })
            
        except Exception as e:
            logger.error(f"Simulation failed for user {user_id}: {str(e)}")
            continue
    
    # ê²°ê³¼ ì €ì¥
    df = pd.DataFrame(results)
    df.to_csv('logs/ab_test_results.csv', index=False)
    
    # ê°„ë‹¨í•œ í†µê³„ ì¶œë ¥
    logger.info(f"Group A CTR: {df[df['group']=='A']['clicked'].mean():.2%}")
    logger.info(f"Group B CTR: {df[df['group']=='B']['clicked'].mean():.2%}")
    
    return df
```

**ì²´í¬ë¦¬ìŠ¤íŠ¸**:
- [ ] ìœ ì € ìƒ˜í”Œë§ ë°©ë²• ì„ íƒ (random vs stratified)
- [ ] ìƒ˜í”Œ í¬ê¸° ê²°ì • (1000ëª… ê¶Œì¥)
- [ ] A/B ê·¸ë£¹ í• ë‹¹ ë¹„ìœ¨ ì„¤ì • (50:50)
- [ ] ì§„í–‰ë¥  í‘œì‹œ (tqdm)
- [ ] ì—ëŸ¬ í•¸ë“¤ë§ ë° ë¡œê¹…
- [ ] ì¤‘ê°„ ê²°ê³¼ ì €ì¥ (ì²´í¬í¬ì¸íŠ¸)
- [ ] ì‹œë®¬ë ˆì´ì…˜ ì™„ë£Œ í›„ ê¸°ë³¸ í†µê³„ ì¶œë ¥

**ì²´í¬í¬ì¸íŠ¸**: ì‹œë®¬ë ˆì´ì…˜ ë¡œê·¸ ìƒì„± ì™„ë£Œ

---

## Phase 7: ë¶„ì„ ë° ë¦¬í¬íŒ…

### ëª©í‘œ
ì‹œë®¬ë ˆì´ì…˜ ê²°ê³¼ë¥¼ ë¶„ì„í•˜ê³  ëŒ€ì‹œë³´ë“œë¥¼ êµ¬ì¶•í•©ë‹ˆë‹¤.

### ì„¸ë¶€ ì‘ì—…

#### 7.1 í†µê³„ ë¶„ì„
**íŒŒì¼**: `src/analysis/statistical_tests.py`

```python
from scipy.stats import chi2_contingency

def analyze_ab_test(results_df):
    """
    ì¹´ì´ì œê³± ê²€ì •ìœ¼ë¡œ A/B ê·¸ë£¹ ê°„ CTR ì°¨ì´ ê²€ì¦
    """
    contingency_table = pd.crosstab(
        results_df['group'],
        results_df['clicked']
    )
    
    chi2, p_value, dof, expected = chi2_contingency(contingency_table)
    
    return {
        'chi2': chi2,
        'p_value': p_value,
        'significant': p_value < 0.05
    }
```

#### 7.2 Streamlit ëŒ€ì‹œë³´ë“œ
**íŒŒì¼**: `app.py`

```python
import streamlit as st

st.title('Local-Helix Dashboard')

# ëª¨ë¸ ì„±ëŠ¥ ì§€í‘œ
st.header('Model Performance')
st.metric('NDCG@10', 0.85)
st.metric('Hit Rate@10', 0.72)

# A/B í…ŒìŠ¤íŠ¸ ê²°ê³¼
st.header('A/B Test Results')
results = pd.read_csv('logs/ab_test_results.csv')

col1, col2 = st.columns(2)
with col1:
    st.metric('Group A CTR', f"{results[results['group']=='A']['clicked'].mean():.2%}")
with col2:
    st.metric('Group B CTR', f"{results[results['group']=='B']['clicked'].mean():.2%}")

# í†µê³„ ê²€ì • ê²°ê³¼
stats = analyze_ab_test(results)
st.write(f"P-value: {stats['p_value']:.4f}")
st.write(f"Statistically Significant: {stats['significant']}")
```

**ì²´í¬ë¦¬ìŠ¤íŠ¸**:
- [ ] í˜ì´ì§€ ë ˆì´ì•„ì›ƒ ì„¤ê³„ (ì‚¬ì´ë“œë°”, ë©”ì¸ ì˜ì—­)
- [ ] ëª¨ë¸ ì„±ëŠ¥ ì„¹ì…˜ êµ¬í˜„ (NDCG, Hit Rate)
- [ ] A/B í…ŒìŠ¤íŠ¸ ê²°ê³¼ ì„¹ì…˜ êµ¬í˜„
- [ ] Feature Importance ì‹œê°í™”
- [ ] ì¸í„°ë™í‹°ë¸Œ í•„í„° ì¶”ê°€ (ë‚ ì§œ, ê·¸ë£¹)
- [ ] ë°ì´í„° ìƒˆë¡œê³ ì¹¨ ê¸°ëŠ¥
- [ ] ì°¨íŠ¸ ë‹¤ìš´ë¡œë“œ ê¸°ëŠ¥
- [ ] ë¡œì»¬ ì„œë²„ í…ŒìŠ¤íŠ¸ (streamlit run app.py)

**ì²´í¬í¬ì¸íŠ¸**: Streamlit ëŒ€ì‹œë³´ë“œ ì‹¤í–‰ ì„±ê³µ

---

## Phase 8: ìµœì¢… ë¬¸ì„œí™”

### ëª©í‘œ
í”„ë¡œì íŠ¸ë¥¼ ì™„ì„±í•˜ê³  ëª¨ë“  ë¬¸ì„œë¥¼ ì •ë¦¬í•©ë‹ˆë‹¤.

### ì„¸ë¶€ ì‘ì—…

#### 8.1 README ì—…ë°ì´íŠ¸
- [ ] í”„ë¡œì íŠ¸ ì†Œê°œ
- [ ] ì„¤ì¹˜ ê°€ì´ë“œ
- [ ] ì‚¬ìš© ë°©ë²•
- [ ] ê²°ê³¼ ìš”ì•½
- [ ] ìŠ¤í¬ë¦°ìƒ· ì¶”ê°€

#### 8.2 ìµœì¢… ë³´ê³ ì„œ ì‘ì„±
**íŒŒì¼**: `FINAL_REPORT.md`

- [ ] í”„ë¡œì íŠ¸ ë°°ê²½
- [ ] ê¸°ìˆ ì  ì ‘ê·¼
- [ ] ì‹¤í—˜ ê²°ê³¼
- [ ] ì¸ì‚¬ì´íŠ¸ ë° ê°œì„  ë°©í–¥
- [ ] ì°¸ê³  ë¬¸í—Œ

#### 8.3 ì½”ë“œ ì •ë¦¬
**íŒŒì¼**: ì „ì²´ í”„ë¡œì íŠ¸

**ì½”ë“œ í’ˆì§ˆ ê¸°ì¤€**:
```python
# 1. PEP 8 ì¤€ìˆ˜
# 2. Type Hints ì‚¬ìš©
def process_data(data: pd.DataFrame, threshold: float = 0.5) -> pd.DataFrame:
    """ë°ì´í„° ì²˜ë¦¬ í•¨ìˆ˜
    
    Args:
        data: ì…ë ¥ ë°ì´í„°í”„ë ˆì„
        threshold: í•„í„°ë§ ì„ê³„ê°’
        
    Returns:
        ì²˜ë¦¬ëœ ë°ì´í„°í”„ë ˆì„
    """
    pass

# 3. ìƒìˆ˜ëŠ” ëŒ€ë¬¸ìë¡œ
MAX_CANDIDATES = 100
DEFAULT_TOP_K = 10

# 4. ë§¤ì§ ë„˜ë²„ ì œê±°
# Bad: if score > 0.7:
# Good: if score > CONFIDENCE_THRESHOLD:
```

**ì²´í¬ë¦¬ìŠ¤íŠ¸**:
- [ ] PEP 8 ìŠ¤íƒ€ì¼ ê°€ì´ë“œ ì¤€ìˆ˜ í™•ì¸
- [ ] ëª¨ë“  public í•¨ìˆ˜ì— Docstring ì¶”ê°€ (Google style)
- [ ] Type Hints ì¶”ê°€ (Python 3.10+)
- [ ] Dead Code ì œê±° (ë¯¸ì‚¬ìš© import, í•¨ìˆ˜)
- [ ] ë§¤ì§ ë„˜ë²„ë¥¼ ìƒìˆ˜ë¡œ ë³€ê²½
- [ ] ê¸´ í•¨ìˆ˜ ë¦¬íŒ©í† ë§ (50ì¤„ ì´í•˜ ê¶Œì¥)
- [ ] Linting ì‹¤í–‰ (flake8, pylint)
- [ ] Code Formatting (black)
- [ ] Import ì •ë¦¬ (isort)
- [ ] ìµœì¢… ì½”ë“œ ë¦¬ë·°

**ì²´í¬í¬ì¸íŠ¸**: GitHub ìµœì¢… ì»¤ë°‹

---

## ğŸ“Š Progress Tracking

| Phase | Status | Completion |
|-------|--------|------------|
| Phase 0: ê¸°íš | âœ… ì™„ë£Œ | 100% |
| Phase 1: í™˜ê²½ ì„¤ì • | â³ ëŒ€ê¸° | 0% |
| Phase 2: ë°ì´í„° ì—”ì§€ë‹ˆì–´ë§ | â³ ëŒ€ê¸° | 0% |
| Phase 3: í›„ë³´êµ° ìƒì„± | â³ ëŒ€ê¸° | 0% |
| Phase 4: ë­í‚¹ ëª¨ë¸ | â³ ëŒ€ê¸° | 0% |
| Phase 5: ë¡œì»¬ ì„œë¹™ | â³ ëŒ€ê¸° | 0% |
| Phase 6: ê°€ìƒ ìœ ì € ì‹œë®¬ë ˆì´ì…˜ | â³ ëŒ€ê¸° | 0% |
| Phase 7: ë¶„ì„ ë° ë¦¬í¬íŒ… | â³ ëŒ€ê¸° | 0% |
| Phase 8: ìµœì¢… ë¬¸ì„œí™” | â³ ëŒ€ê¸° | 0% |

---

## ğŸ¯ Success Criteria

í”„ë¡œì íŠ¸ ì„±ê³µ ê¸°ì¤€:
- [ ] 3ì²œë§Œ ê±´ ë°ì´í„° ì²˜ë¦¬ ì™„ë£Œ
- [ ] LightGBM ëª¨ë¸ NDCG@10 > 0.7
- [ ] A/B í…ŒìŠ¤íŠ¸ì—ì„œ í†µê³„ì  ìœ ì˜ì„± í™•ë³´ (p < 0.05)
- [ ] Streamlit ëŒ€ì‹œë³´ë“œ ì •ìƒ ì‘ë™
- [ ] ì „ì²´ ì½”ë“œ GitHub ê³µê°œ

---

**Last Updated**: 2025-12-24  
**Next Review**: Phase 1 ì™„ë£Œ í›„
